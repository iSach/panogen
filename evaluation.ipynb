{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook contains the codes for performance assessment of our different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion detection\n",
    "\n",
    "Here, the performance of our motion detector is assessed.\n",
    "\n",
    "The module takes as input a frame, and outputs a corresponding mask where the background is 0, and the foreground is 1.\n",
    "\n",
    "We evaluate our model on annotations done specifically in the public image database of the project.\n",
    "\n",
    "The metric we use is simply Pixel Accuracy, the ratio of pixels correctly classified over all pixels. A value of 1 thus corresponds to 100% accuracy (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sach/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n",
      "Inside accuracy: 85.4%\n",
      "Outside accuracy: 98.6%\n",
      "Total accuracy: 92.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import numpy as np\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "def eval_md_accuracy(scene):\n",
    "\t\"\"\"\n",
    "\tparams:\n",
    "\t\tscene: scene id (1 for inside, 2 for outside)\n",
    "\t\"\"\"\n",
    "\taccuracy = 0\n",
    "\tfor id in label_ids[scene - 1]:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\t\tpred = md.detect(img)['mask']\n",
    "\n",
    "\t\t# Read ground truth\n",
    "\t\timg_path = os.path.join('data', 'bb_img_5_{}'.format(scene), 'seg_5_{}_{}.png'.format(scene, str(id).zfill(4)))\n",
    "\t\tmask_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) / 255\n",
    "\n",
    "\t\t# Calculate accuracy\n",
    "\t\tacc = np.sum(pred == mask_img) / (720 * 1280)\n",
    "\t\taccuracy += acc\n",
    "\n",
    "\treturn accuracy / len(label_ids[scene - 1])\n",
    "\n",
    "md = MotionDetector(model='yolov5x', ball_mask='square', person_mask='rectangle')\n",
    "\n",
    "inside_acc = eval_md_accuracy(1) * 100\n",
    "outside_acc = eval_md_accuracy(2) * 100\n",
    "print('Inside accuracy: {}%'.format(inside_acc.round(2)))\n",
    "print('Outside accuracy: {}%'.format(outside_acc.round(2)))\n",
    "print('Total accuracy: {}%'.format(((inside_acc + outside_acc) / 2).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "Here, the performance of our object detector is assessed.\n",
    "\n",
    "The model takes as input a frame, and outputs a list of 5-value vectors.\n",
    "Each vector contains the x1, y1, x2, y2 coordinates of the bounding box and the 5th value is the class (0 = person, 1 = ball).\n",
    "\n",
    "Following the literature, we use the mAP (mean Average Precision) for evaluating our model. We compute the AP for both classes and then averaged.\n",
    "Higher is better, again.\n",
    "\n",
    "Once again, the reference data is the public image database of the project annotated for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nvdia/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n",
      "Average inference time: 0.5432843399047852s\n",
      "Average inference time: 0.5287575920422872s\n",
      "Inside mAP: 81.99%\n",
      "Inside mAP (person): 89.5%\n",
      "Inside mAP (ball): 74.47%\n",
      "Outside mAP: 61.26%\n",
      "Outside mAP (person): 95.25%\n",
      "Outside mAP (ball): 27.28%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "\n",
    "def eval_map(scene, debug_inf_time=True, rgb=True, size=640):\n",
    "\t# Read reference data and build dictionary\n",
    "\tids = label_ids[scene - 1]\n",
    "\n",
    "\t# Read file box_5_1.txt as csv\n",
    "\tdf = pd.read_csv(os.path.join('data', 'box_5_{}.txt'.format(scene)), header=None, sep=', ', engine='python')\n",
    "\tdf.columns = ['id', 'x1', 'y1', 'x2', 'y2', 'class']\n",
    "\tdf.replace({'class': {'person': 0, 'ball': 1}}, inplace=True)\n",
    "\tdf['id'] = df['id'].apply(lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "\tpreds = []\n",
    "\ttarget = []\n",
    "\tinf_time = []\n",
    "\n",
    "\tfor id in ids:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_COLOR if rgb else cv2.IMREAD_GRAYSCALE)\n",
    "\t\tt = time.time()\n",
    "\t\tpred = md.detect(img, size=size)['boxes']\n",
    "\t\tinf_time.append(time.time() - t)\n",
    "\t\t# pred is a tensor of shape (N, 6) where N is the number of bounding boxes, and the 6 values are (x1, y1, x2, y2, conf, class)\n",
    "\n",
    "\t\tboxes = pred[:, :4]\n",
    "\t\tscores = pred[:, 4]\n",
    "\t\tlabels = pred[:, 5].int()\n",
    "\n",
    "\t\tpreds.append({'boxes': boxes, 'scores': scores, 'labels': labels})\n",
    "\n",
    "\t\t# Get ground truth\n",
    "\t\tgt = df[df['id'] == id]\n",
    "\t\tgt = torch.from_numpy(gt[['x1', 'y1', 'x2', 'y2', 'class']].values).to(device)\n",
    "\n",
    "\t\tboxes = gt[:, :4]\n",
    "\t\tlabels = gt[:, 4].int()\n",
    "\n",
    "\t\ttarget.append({'boxes': boxes, 'labels': labels})\n",
    "\t\t\n",
    "\tmetric = MeanAveragePrecision(iou_thresholds=[0.5], class_metrics=True).to(device)\n",
    "\tmetric.update(preds, target)\n",
    "\tresults = metric.compute()\n",
    "\n",
    "\tif debug_inf_time:\n",
    "\t\tprint('Average inference time: {}s'.format(np.mean(inf_time)))\n",
    "\n",
    "\treturn (results['map_50'].item(), *results['map_per_class'].tolist())\n",
    "\n",
    "md = MotionDetector(model='yolov5x')\n",
    "rgb = True\n",
    "size = 640\n",
    "inside_map, inside_map_person, inside_map_ball = eval_map(1, rgb=rgb, size=size)\n",
    "outside_map, outside_map_person, outside_map_ball = eval_map(2, rgb=rgb, size=size)\n",
    "print('Inside mAP: {}%'.format(round(inside_map * 100, 2)))\n",
    "print('Inside mAP (person): {}%'.format(round(inside_map_person * 100, 2)))\n",
    "print('Inside mAP (ball): {}%'.format(round(inside_map_ball * 100, 2)))\n",
    "print('Outside mAP: {}%'.format(round(outside_map * 100, 2)))\n",
    "print('Outside mAP (person): {}%'.format(round(outside_map_person * 100, 2)))\n",
    "print('Outside mAP (ball): {}%'.format(round(outside_map_ball * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed\n",
    "\n",
    "This module helps to compare the speed of the different detection models we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nvdia/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nvdia/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n",
      "Average inference time for models:\n",
      "yolov5n: 62.18ms (16.08fps)\n",
      "yolov5s: 72.25ms (13.84fps)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "from motion_detection import MODELS\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "\n",
    "path = \"data/img_5_1/img_5_1_\"\n",
    "nb_frames = 500\n",
    "\n",
    "avg_times = dict()\n",
    "for model in ['yolov5n', 'yolov5s']: \n",
    "    md = MotionDetector(model=model)\n",
    "    inf_times = []\n",
    "    for i in range(0, nb_frames):\n",
    "        img = cv2.imread(path + str(i).zfill(4) + \".jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "        t = time.time()\n",
    "        md.detect(img, size=320)\n",
    "        inf_times.append(time.time() - t)\n",
    "    avg_times[model] = (np.mean(inf_times) * 1000).round(2)\n",
    "\n",
    "print('Average inference time for models:')\n",
    "for model, avg_time in avg_times.items():\n",
    "    print('{}: {}ms ({}fps)'.format(model, avg_time, round(1000 / avg_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ball Tracking\n",
    "\n",
    "Here, the performance of the object tracker is assessed.\n",
    "\n",
    "The tracker outputs at each frame the position and depth ((x,y) and z) for the ball(s).\n",
    "\n",
    "The ball center on the x and y axes is supposed to be the center of the bounding box. For performance about this, refer to Object Detection, which evaluates those bounding boxes.\n",
    "\n",
    "***TODO*** For the depth, we record some frames and measure the physical depth, and then average the distance between the predicted and ground truth depth on all annotated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvdia/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /home/nvdia/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2022-12-3 Python-3.6.9 torch-1.10.0 CUDA:0 (NVIDIA Tegra X2, 7850MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from motion_detection import MotionDetector\n",
    "import cv2\n",
    "\n",
    "files_path = 'data/ball_pos/'\n",
    "\n",
    "md = MotionDetector(model='yolov5n')\n",
    "\n",
    "for file in list(glob.glob(files_path + '*.jpg')):\n",
    "    frame = cv2.imread(file)\n",
    "\n",
    "    bboxes = md.detect(frame)['boxes']\n",
    "    bboxes = bboxes[bboxes[:, 5] == 1][:, :4]\n",
    "\n",
    "    ball_depths = md.compute_ball_depths(bboxes)\n",
    "\n",
    "    # Draw bounding boxes and depth\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        is_big, depth = ball_depths[i]\n",
    "        x1, y1, x2, y2 = bbox.int()\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        txt = 'BALL' if is_big else 'ball'\n",
    "        txt_z = '{}m'.format(round(depth, 2))\n",
    "        cv2.putText(frame, txt, (x1, y1 + 16), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 0, 0), 2)\n",
    "        cv2.putText(frame, txt_z, (x1, y1 + 32), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(file, frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c2e117626dba50c6b1b13c3df5f8cd020e3b4688fec6edea3e06d08349081b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
