{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook contains the codes for performance assessment of our different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion detection\n",
    "\n",
    "Here, the performance of our motion detector is assessed.\n",
    "\n",
    "The module takes as input a frame, and outputs a corresponding mask where the background is 0, and the foreground is 1.\n",
    "\n",
    "We evaluate our model on annotations done specifically in the public image database of the project.\n",
    "\n",
    "The metric we use is simply Pixel Accuracy, the ratio of pixels correctly classified over all pixels. A value of 1 thus corresponds to 100% accuracy (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sacha/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-12-2 Python-3.9.0 torch-1.13.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside accuracy: 86.82%\n",
      "Outside accuracy: 98.71%\n",
      "Total accuracy: 92.76%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import numpy as np\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "\n",
    "md = MotionDetector()\n",
    "\n",
    "def eval_md_accuracy(scene):\n",
    "\t\"\"\"\n",
    "\tparams:\n",
    "\t\tscene: scene id (1 for inside, 2 for outside)\n",
    "\t\"\"\"\n",
    "\taccuracy = 0\n",
    "\tfor id in label_ids[scene - 1]:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\t\tpred = md.detect(img)\n",
    "\n",
    "\t\t# Read ground truth\n",
    "\t\timg_path = os.path.join('data', 'bb_img_5_{}'.format(scene), 'seg_5_{}_{}.png'.format(scene, str(id).zfill(4)))\n",
    "\t\tmask_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) / 255\n",
    "\n",
    "\t\t# Calculate accuracy\n",
    "\t\tacc = np.sum(pred == mask_img) / (720 * 1280)\n",
    "\t\taccuracy += acc\n",
    "\n",
    "\treturn accuracy / len(label_ids[scene - 1])\n",
    "\t\n",
    "inside_acc = eval_md_accuracy(1) * 100\n",
    "outside_acc = eval_md_accuracy(2) * 100\n",
    "print('Inside accuracy: {}%'.format(inside_acc.round(2)))\n",
    "print('Outside accuracy: {}%'.format(outside_acc.round(2)))\n",
    "print('Total accuracy: {}%'.format(((inside_acc + outside_acc) / 2).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "Here, the performance of our object detector is assessed.\n",
    "\n",
    "The model takes as input a frame, and outputs a list of 5-value vectors.\n",
    "Each vector contains the x1, y1, x2, y2 coordinates of the bounding box and the 5th value is the class (0 = person, 1 = ball).\n",
    "\n",
    "Following the literature, we use the mAP (mean Average Precision) for evaluating our model. We compute the AP for both classes and then averaged.\n",
    "Higher is better, again.\n",
    "\n",
    "Once again, the reference data is the public image database of the project annotated for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ball Tracking\n",
    "\n",
    "Here, the performance of the object tracker is assessed.\n",
    "\n",
    "The tracker outputs at each frame the position and depth ((x,y) and z) for the ball(s).\n",
    "\n",
    "The ball center on the x and y axes is supposed to be the center of the bounding box. For performance about this, refer to Object Detection, which evaluates those bounding boxes.\n",
    "\n",
    "***TODO*** For the depth, we record some frames and measure the physical depth, and then average the distance between the predicted and ground truth depth on all annotated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tfe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f894e437981a62ea28966f86d5a604c45536486bc8d6145cb6a9cc0389df8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
