{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook contains the codes for performance assessment of our different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion detection\n",
    "\n",
    "Here, the performance of our motion detector is assessed.\n",
    "\n",
    "The module takes as input a frame, and outputs a corresponding mask where the background is 0, and the foreground is 1.\n",
    "\n",
    "We evaluate our model on annotations done specifically in the public image database of the project.\n",
    "\n",
    "The metric we use is simply Pixel Accuracy, the ratio of pixels correctly classified over all pixels. A value of 1 thus corresponds to 100% accuracy (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sach/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-11-30 Python-3.6.9 torch-1.10.1 CUDA:0 (NVIDIA GeForce RTX 3080, 10015MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n",
      "Inside accuracy: 86.83%\n",
      "Outside accuracy: 98.71%\n",
      "Total accuracy: 92.77%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import numpy as np\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "\n",
    "md = MotionDetector()\n",
    "\n",
    "def eval_md_accuracy(scene):\n",
    "\t\"\"\"\n",
    "\tparams:\n",
    "\t\tscene: scene id (1 for inside, 2 for outside)\n",
    "\t\"\"\"\n",
    "\taccuracy = 0\n",
    "\tfor id in label_ids[scene - 1]:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\t\tpred = md.detect(img)['mask']\n",
    "\n",
    "\t\t# Read ground truth\n",
    "\t\timg_path = os.path.join('data', 'bb_img_5_{}'.format(scene), 'seg_5_{}_{}.png'.format(scene, str(id).zfill(4)))\n",
    "\t\tmask_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) / 255\n",
    "\n",
    "\t\t# Calculate accuracy\n",
    "\t\tacc = np.sum(pred == mask_img) / (720 * 1280)\n",
    "\t\taccuracy += acc\n",
    "\n",
    "\treturn accuracy / len(label_ids[scene - 1])\n",
    "\t\n",
    "inside_acc = eval_md_accuracy(1) * 100\n",
    "outside_acc = eval_md_accuracy(2) * 100\n",
    "print('Inside accuracy: {}%'.format(inside_acc.round(2)))\n",
    "print('Outside accuracy: {}%'.format(outside_acc.round(2)))\n",
    "print('Total accuracy: {}%'.format(((inside_acc + outside_acc) / 2).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "Here, the performance of our object detector is assessed.\n",
    "\n",
    "The model takes as input a frame, and outputs a list of 5-value vectors.\n",
    "Each vector contains the x1, y1, x2, y2 coordinates of the bounding box and the 5th value is the class (0 = person, 1 = ball).\n",
    "\n",
    "Following the literature, we use the mAP (mean Average Precision) for evaluating our model. We compute the AP for both classes and then averaged.\n",
    "Higher is better, again.\n",
    "\n",
    "Once again, the reference data is the public image database of the project annotated for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sach/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-11-30 Python-3.6.9 torch-1.10.1 CUDA:0 (NVIDIA GeForce RTX 3080, 10015MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.00000e+00, 7.14041e+00, 2.04462e+02, 6.69243e+02, 2.72891e-01, 0.00000e+00]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5deab291325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0meval_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-b5deab291325>\u001b[0m in \u001b[0;36meval_map\u001b[0;34m(scene)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0;31m# Calculate the average precision for class c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                         \u001b[0mAPs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_ap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b5deab291325>\u001b[0m in \u001b[0;36mcalculate_ap\u001b[0;34m(preds, gt)\u001b[0m\n\u001b[1;32m     44\u001b[0m \t\"\"\"\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Sort predictions by confidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'confidence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Calculate the number of true positives and false positives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "md = MotionDetector()\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "\n",
    "iou_threshold = 0.5\n",
    "\n",
    "def iou(bb1, bb2):\n",
    "\t\"\"\"\n",
    "\tparams:\n",
    "\t\tbb1: bounding box 1\n",
    "\t\tbb2: bounding box 2\n",
    "\t\"\"\"\n",
    "\t# Calculate the intersection\n",
    "\tx1 = max(bb1[0], bb2[0])\n",
    "\ty1 = max(bb1[1], bb2[1])\n",
    "\tx2 = min(bb1[2], bb2[2])\n",
    "\ty2 = min(bb1[3], bb2[3])\n",
    "\tintersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "\t# Calculate the union\n",
    "\tarea1 = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "\tarea2 = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "\tunion = area1 + area2 - intersection\n",
    "\n",
    "\treturn intersection / union\n",
    "\n",
    "def calculate_ap(preds, gt):\n",
    "\t\"\"\"\n",
    "\tComputes the AP for a given class between two sets of bounding boxes\n",
    "\n",
    "\tparams:\n",
    "\t\tpreds: predicted bounding boxes (x1, y1, x2, y2, conf)\n",
    "\t\tgt: ground truth bounding boxes (x1, y1, x2, y2)\n",
    "\t\"\"\"\n",
    "\t# Sort predictions by confidence\n",
    "\tpreds = preds.sort_values(by='confidence', ascending=False)\n",
    "\n",
    "\t# Calculate the number of true positives and false positives\n",
    "\ttp = []\n",
    "\tfp = []\n",
    "\tfor _, pred in preds.iterrows():\n",
    "\t\tmax_iou = 0\n",
    "\t\tfor _, bb in gt.iterrows():\n",
    "\t\t\tmax_iou = max(max_iou, iou(pred['bb'], bb['bb']))\n",
    "\n",
    "\t\tif max_iou >= iou_threshold:\n",
    "\t\t\ttp.append(1)\n",
    "\t\t\tfp.append(0)\n",
    "\t\telse:\n",
    "\t\t\ttp.append(0)\n",
    "\t\t\tfp.append(1)\n",
    "\n",
    "\t# Calculate the precision and recall\n",
    "\ttp = np.array(tp)\n",
    "\tfp = np.array(fp)\n",
    "\tprecision = np.cumsum(tp) / (np.cumsum(tp) + np.cumsum(fp))\n",
    "\trecall = np.cumsum(tp) / len(gt)\n",
    "\n",
    "\t# Calculate the AP\n",
    "\tap = 0\n",
    "\tfor i in range(1, len(precision)):\n",
    "\t\tap += (recall[i] - recall[i - 1]) * precision[i]\n",
    "\n",
    "\treturn ap\n",
    "\t\n",
    "\n",
    "def eval_map(scene):\n",
    "\t# Read reference data and build dictionary\n",
    "\tids = label_ids[scene - 1]\n",
    "\n",
    "\t# Read file box_5_1.txt as csv\n",
    "\tdf = pd.read_csv(os.path.join('data', 'box_5_{}.txt'.format(scene)), header=None, sep=', ', engine='python')\n",
    "\tdf.columns = ['id', 'x1', 'y1', 'x2', 'y2', 'class']\n",
    "\tdf.replace({'class': {'person': 0, 'ball': 1}}, inplace=True)\n",
    "\tdf['id'] = df['id'].apply(lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "\t# APs for each class\n",
    "\tAPs = [0, 0]\n",
    "\n",
    "\tfor id in ids:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\t\tpred = md.detect(img)\n",
    "\n",
    "\t\t# Get ground truth\n",
    "\t\tgt = df[df['id'] == id]\n",
    "\t\tgt = torch.tensor(gt[['x1', 'y1', 'x2', 'y2', 'class']].values).to(device)\n",
    "\n",
    "\t\t# Calculate the average precision for each class\n",
    "\t\tfor class_id, class_name in enumerate(['person', 'ball']):\n",
    "\t\t\t# Get the predicted bounding boxes for class c\n",
    "\t\t\tpred_c = pred[class_name + '_boxes']\n",
    "\t\t\tprint(pred_c)\n",
    "\n",
    "\t\t\t# Get the ground truth bounding boxes for class c\n",
    "\t\t\tgt_c = gt[gt[:, 4] == class_id][:, :4]\n",
    "\n",
    "\t\t\t# Calculate the average precision for class c\n",
    "\t\t\tAPs[class_id] += calculate_ap(pred_c, gt_c)\n",
    "\t\t\n",
    "\n",
    "eval_map(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "preds = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]),\n",
    "    scores=torch.tensor([0.536]),\n",
    "    labels=torch.tensor([0]),\n",
    "  )\n",
    "]\n",
    "target = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),\n",
    "    labels=torch.tensor([0]),\n",
    "  )\n",
    "]\n",
    "metric = MeanAveragePrecision()\n",
    "metric.update(preds, target)\n",
    "from pprint import pprint\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ball Tracking\n",
    "\n",
    "Here, the performance of the object tracker is assessed.\n",
    "\n",
    "The tracker outputs at each frame the position and depth ((x,y) and z) for the ball(s).\n",
    "\n",
    "The ball center on the x and y axes is supposed to be the center of the bounding box. For performance about this, refer to Object Detection, which evaluates those bounding boxes.\n",
    "\n",
    "***TODO*** For the depth, we record some frames and measure the physical depth, and then average the distance between the predicted and ground truth depth on all annotated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c2e117626dba50c6b1b13c3df5f8cd020e3b4688fec6edea3e06d08349081b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
