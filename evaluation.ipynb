{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook contains the codes for performance assessment of our different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion detection\n",
    "\n",
    "Here, the performance of our motion detector is assessed.\n",
    "\n",
    "The module takes as input a frame, and outputs a corresponding mask where the background is 0, and the foreground is 1.\n",
    "\n",
    "We evaluate our model on annotations done specifically in the public image database of the project.\n",
    "\n",
    "The metric we use is simply Pixel Accuracy, the ratio of pixels correctly classified over all pixels. A value of 1 thus corresponds to 100% accuracy (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sach/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-11-30 Python-3.6.9 torch-1.10.1 CUDA:0 (NVIDIA GeForce RTX 3080, 10015MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside accuracy: 89.15%\n",
      "Outside accuracy: 98.99%\n",
      "Total accuracy: 94.07%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import numpy as np\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "def eval_md_accuracy(scene):\n",
    "\t\"\"\"\n",
    "\tparams:\n",
    "\t\tscene: scene id (1 for inside, 2 for outside)\n",
    "\t\"\"\"\n",
    "\taccuracy = 0\n",
    "\tfor id in label_ids[scene - 1]:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\t\tpred = md.detect(img)['mask']\n",
    "\n",
    "\t\t# Read ground truth\n",
    "\t\timg_path = os.path.join('data', 'bb_img_5_{}'.format(scene), 'seg_5_{}_{}.png'.format(scene, str(id).zfill(4)))\n",
    "\t\tmask_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) / 255\n",
    "\n",
    "\t\t# Calculate accuracy\n",
    "\t\tacc = np.sum(pred == mask_img) / (720 * 1280)\n",
    "\t\taccuracy += acc\n",
    "\n",
    "\treturn accuracy / len(label_ids[scene - 1])\n",
    "\n",
    "md = MotionDetector(model='yolov5x')\n",
    "\n",
    "inside_acc = eval_md_accuracy(1) * 100\n",
    "outside_acc = eval_md_accuracy(2) * 100\n",
    "print('Inside accuracy: {}%'.format(inside_acc.round(2)))\n",
    "print('Outside accuracy: {}%'.format(outside_acc.round(2)))\n",
    "print('Total accuracy: {}%'.format(((inside_acc + outside_acc) / 2).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "Here, the performance of our object detector is assessed.\n",
    "\n",
    "The model takes as input a frame, and outputs a list of 5-value vectors.\n",
    "Each vector contains the x1, y1, x2, y2 coordinates of the bounding box and the 5th value is the class (0 = person, 1 = ball).\n",
    "\n",
    "Following the literature, we use the mAP (mean Average Precision) for evaluating our model. We compute the AP for both classes and then averaged.\n",
    "Higher is better, again.\n",
    "\n",
    "Once again, the reference data is the public image database of the project annotated for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nvdia/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-12-3 Python-3.6.9 torch-1.10.0 CUDA:0 (NVIDIA Tegra X2, 7850MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Python 3.7.0 is required by YOLOv5, but Python 3.6.9 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[130.33037,  75.28920, 427.97632, 287.91821]], device='cuda:0'), 'scores': tensor([0.56999], device='cuda:0'), 'labels': tensor([1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 826.62195,   11.83264, 1138.22327,  706.82214]], device='cuda:0'), 'scores': tensor([0.86603], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([], device='cuda:0', size=(0, 4)), 'scores': tensor([], device='cuda:0'), 'labels': tensor([], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 73.66552,  70.79117, 297.99969, 692.96436],\n",
      "        [220.15088, 320.62335, 258.97333, 349.94537]], device='cuda:0'), 'scores': tensor([0.85955, 0.56130], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 92.70454,  52.25848, 389.29022, 698.84503]], device='cuda:0'), 'scores': tensor([0.92556], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 24.54118,  45.62701, 301.72327, 710.73883]], device='cuda:0'), 'scores': tensor([0.91331], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[1184.59619,  127.18405, 1280.00000,  718.35706],\n",
      "        [   0.00000,   38.22574,  107.28239,  471.17880]], device='cuda:0'), 'scores': tensor([0.52782, 0.44175], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[6.78822e+01, 1.16406e+00, 3.47844e+02, 6.95610e+02],\n",
      "        [1.20310e+03, 1.31768e+02, 1.28000e+03, 7.16760e+02]], device='cuda:0'), 'scores': tensor([0.91907, 0.39460], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[9.86516e+02, 1.60894e+01, 1.27985e+03, 4.05960e+02],\n",
      "        [6.22375e-01, 2.23105e+02, 1.75428e+02, 4.31673e+02]], device='cuda:0'), 'scores': tensor([0.64451, 0.44815], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  0.00000,  45.77710, 224.85281, 695.35773]], device='cuda:0'), 'scores': tensor([0.92337], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  0.00000,  55.72855, 314.95154, 701.60535]], device='cuda:0'), 'scores': tensor([0.85256], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 21.07542,  79.71527, 287.47101, 660.76147],\n",
      "        [ 81.98841, 321.43359, 224.48291, 402.24762]], device='cuda:0'), 'scores': tensor([0.71332, 0.50667], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[220.83354,  20.45041, 502.35358, 688.92957],\n",
      "        [157.81143,  64.88264, 289.80319, 185.52095]], device='cuda:0'), 'scores': tensor([0.82022, 0.46681], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[307.26349,   2.34351, 924.29132, 720.00000],\n",
      "        [772.32617,  66.73320, 917.37659, 209.30531],\n",
      "        [623.27319, 257.23288, 678.92554, 322.86026]], device='cuda:0'), 'scores': tensor([0.53120, 0.27092, 0.25076], device='cuda:0'), 'labels': tensor([0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 119.37180,   20.22491,  707.70654,  698.88245],\n",
      "        [ 783.36664,    0.00000, 1105.08911,  710.27917],\n",
      "        [ 109.26687,  319.15295,  220.54208,  411.97589],\n",
      "        [ 773.79120,   40.53110,  883.54974,  157.22284]], device='cuda:0'), 'scores': tensor([0.82620, 0.74941, 0.67414, 0.28348], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 838.62152,   10.35150, 1120.53906,  710.45453],\n",
      "        [ 877.73114,  244.61273,  964.26178,  351.82574]], device='cuda:0'), 'scores': tensor([0.78830, 0.34271], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 825.33850,   28.27097, 1161.98975,  710.81494]], device='cuda:0'), 'scores': tensor([0.54137], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 803.48621,    0.00000, 1176.05078,  717.79242],\n",
      "        [ 765.99561,  265.05417,  894.26416,  411.96518]], device='cuda:0'), 'scores': tensor([0.77349, 0.57990], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 661.77930,    5.24033, 1210.04907,  714.30884]], device='cuda:0'), 'scores': tensor([0.50429], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  1.86411,   3.02090, 345.91867, 698.63098],\n",
      "        [233.99187, 162.84174, 315.27734, 251.31396]], device='cuda:0'), 'scores': tensor([0.88526, 0.60106], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  40.21640,    3.85104,  451.75241,  709.81628],\n",
      "        [1122.82605,   62.00470, 1279.67126,  708.96002]], device='cuda:0'), 'scores': tensor([0.75517, 0.56593], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 16.41640,   4.36893, 518.21014, 689.06848],\n",
      "        [434.59811, 130.32280, 639.28174, 349.70264]], device='cuda:0'), 'scores': tensor([0.88080, 0.84847], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[629.41113,   7.97290, 999.24072, 713.55145]], device='cuda:0'), 'scores': tensor([0.67075], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([], device='cuda:0', size=(0, 4)), 'scores': tensor([], device='cuda:0'), 'labels': tensor([], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  1.68288,  18.83881, 197.39507, 691.29553]], device='cuda:0'), 'scores': tensor([0.61153], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}]\n",
      "[{'boxes': tensor([[  0,   0, 258, 702],\n",
      "        [128,  80, 440, 341]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 324,    0, 1158,  720]], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[1092,    0, 1280,  720],\n",
      "        [1059,  237, 1193,  350]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 62,  60, 310, 720],\n",
      "        [207, 309, 266, 370]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 81,  53, 400, 705],\n",
      "        [344, 326, 396, 380]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 24,  42, 312, 700]], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[   0,   25,  125,  721],\n",
      "        [ 925,   27, 1016,  130],\n",
      "        [1015,  153, 1280,  266],\n",
      "        [ 990,  150, 1280,  720]], device='cuda:0'), 'labels': tensor([0, 1, 1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  66,    0,  353,  702],\n",
      "        [1127,  190, 1280,  720],\n",
      "        [1134,  303, 1193,  361],\n",
      "        [1240,  146, 1280,  235]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[   0,  220,  171,  428],\n",
      "        [ 273,  225,  380,  279],\n",
      "        [ 978,    0, 1280,  540]], device='cuda:0'), 'labels': tensor([0, 1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  0,  30, 220, 700],\n",
      "        [147, 320, 203, 373]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 142,  284,  200,  331],\n",
      "        [ 517,  173,  687,  289],\n",
      "        [   0,   53,  323,  698],\n",
      "        [1110,  258, 1280,  416]], device='cuda:0'), 'labels': tensor([1, 1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 41, 345,  97, 391],\n",
      "        [ 94, 320, 204, 421],\n",
      "        [ 32,  48, 281, 697]], device='cuda:0'), 'labels': tensor([1, 1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 208,    0,  272,   52],\n",
      "        [ 148,   62,  287,  183],\n",
      "        [ 128,   25,  498,  700],\n",
      "        [1251,   79, 1280,  378]], device='cuda:0'), 'labels': tensor([1, 1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 621,  261,  674,  315],\n",
      "        [ 764,   79,  921,  221],\n",
      "        [ 324,    0,  924,  702],\n",
      "        [1116,    0, 1280,  720]], device='cuda:0'), 'labels': tensor([1, 1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 105,  320,  217,  410],\n",
      "        [ 772,   40,  887,  153],\n",
      "        [ 101,    0,  707,  701],\n",
      "        [ 781,    0, 1108,  720]], device='cuda:0'), 'labels': tensor([1, 1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[   0,  132,  254,  201],\n",
      "        [ 850,    0, 1110,  719],\n",
      "        [ 252,  141,  307,  200],\n",
      "        [ 871,  246,  992,  362]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 769,    0,  876,   96],\n",
      "        [ 822,    0, 1165,  719]], device='cuda:0'), 'labels': tensor([1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 768,  273,  886,  379],\n",
      "        [ 867,    0, 1162,  719]], device='cuda:0'), 'labels': tensor([1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[   0,  231,  138,  592],\n",
      "        [ 644,    0, 1207,  719],\n",
      "        [  70,  178,  147,  239],\n",
      "        [ 300,  113,  442,  232]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[   0,    0,  344,  719],\n",
      "        [1069,    0, 1279,  719],\n",
      "        [ 240,  166,  311,  234],\n",
      "        [   0,  105,  140,  263]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  48,    0,  425,  705],\n",
      "        [1157,    0, 1280,  720],\n",
      "        [ 210,   63,  299,  135]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[   0,    0,  609,  720],\n",
      "        [1223,  140, 1280,  287],\n",
      "        [ 440,  134,  627,  333]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 590,    0,  980,  720],\n",
      "        [1200,   96, 1280,  720],\n",
      "        [ 404,  219,  621,  443]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([], device='cuda:0', size=(0, 4), dtype=torch.int64), 'labels': tensor([], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  0,   0, 206, 720],\n",
      "        [ 84, 201, 233, 350]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}]\n",
      "[{'boxes': tensor([[ 63.02713, 418.89600, 234.69070, 709.32202],\n",
      "        [288.20474,  80.94915, 337.49777, 135.51505]], device='cuda:0'), 'scores': tensor([0.81807, 0.41525], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 322.34470,  378.27951,  456.63126,  682.10962],\n",
      "        [ 967.79431,  349.95691, 1060.15015,  650.46533]], device='cuda:0'), 'scores': tensor([0.91620, 0.82750], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[362.76294, 374.92020, 441.12610, 615.55023],\n",
      "        [829.95258, 350.00284, 912.66522, 608.05457]], device='cuda:0'), 'scores': tensor([0.87930, 0.82042], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[293.91895, 345.09869, 368.61792, 578.76764],\n",
      "        [682.13586, 359.42032, 767.44763, 601.73041]], device='cuda:0'), 'scores': tensor([0.85965, 0.82508], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[272.71420, 376.01117, 344.42593, 560.88464],\n",
      "        [636.53979, 350.72632, 743.92236, 599.45178],\n",
      "        [240.97389, 509.62311, 269.06314, 542.29938]], device='cuda:0'), 'scores': tensor([0.88921, 0.85470, 0.49683], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[645.84229, 356.51007, 711.13928, 602.00854],\n",
      "        [ 75.70232, 386.31180, 179.90320, 603.43744],\n",
      "        [ 23.80929, 595.20831,  63.91564, 629.46906]], device='cuda:0'), 'scores': tensor([0.81824, 0.81753, 0.70041], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[544.94495, 372.79694, 614.85547, 591.34381]], device='cuda:0'), 'scores': tensor([0.78780], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[423.04529, 363.59232, 505.22858, 585.07758]], device='cuda:0'), 'scores': tensor([0.82448], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[329.72989, 345.18469, 434.17282, 588.19098]], device='cuda:0'), 'scores': tensor([0.83110], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[521.66559, 356.20746, 599.87262, 603.42322]], device='cuda:0'), 'scores': tensor([0.80404], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 69.21039, 390.54895, 193.58002, 642.33887],\n",
      "        [623.24042, 353.40765, 702.43390, 604.07935]], device='cuda:0'), 'scores': tensor([0.89073, 0.82462], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[204.25177, 403.69696, 274.31213, 598.99042],\n",
      "        [696.40381, 371.70953, 778.97888, 622.37286]], device='cuda:0'), 'scores': tensor([0.85353, 0.79384], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[501.92780, 394.68399, 560.42621, 518.16870],\n",
      "        [750.91418, 384.48572, 793.85315, 511.36914]], device='cuda:0'), 'scores': tensor([0.83758, 0.79845], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[738.93243, 393.83762, 805.03265, 505.87988],\n",
      "        [879.09424, 384.04053, 928.83655, 515.94067]], device='cuda:0'), 'scores': tensor([0.86887, 0.83433], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 964.38879,  360.96915, 1022.35059,  469.80234],\n",
      "        [ 448.50070,  396.15012,  487.16812,  505.47784]], device='cuda:0'), 'scores': tensor([0.81585, 0.56178], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[820.20593, 357.73291, 849.89526, 458.62378],\n",
      "        [292.45612, 387.93121, 338.69012, 509.92096]], device='cuda:0'), 'scores': tensor([0.78467, 0.76705], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  0.00000, 375.88736,  49.56475, 519.21436],\n",
      "        [543.64502, 368.92123, 568.50610, 445.77988]], device='cuda:0'), 'scores': tensor([0.73704, 0.36557], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 35.99574, 395.27289, 137.43288, 523.33508],\n",
      "        [631.23199, 356.57663, 667.90607, 459.34488]], device='cuda:0'), 'scores': tensor([0.78480, 0.75871], device='cuda:0'), 'labels': tensor([0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[481.57434, 360.11523, 525.04016, 445.65161]], device='cuda:0'), 'scores': tensor([0.66018], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[362.43134, 343.67139, 405.44104, 449.55859]], device='cuda:0'), 'scores': tensor([0.81383], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[391.45151, 344.85907, 443.71970, 451.97302]], device='cuda:0'), 'scores': tensor([0.74350], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[558.44476, 351.86334, 604.24567, 424.36450]], device='cuda:0'), 'scores': tensor([0.41259], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[897.07373, 345.56656, 945.02209, 473.46494]], device='cuda:0'), 'scores': tensor([0.83321], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[946.59900, 346.81284, 997.21912, 477.64584]], device='cuda:0'), 'scores': tensor([0.84918], device='cuda:0'), 'labels': tensor([0], device='cuda:0', dtype=torch.int32)}]\n",
      "[{'boxes': tensor([[ 60, 401, 246, 720],\n",
      "        [267,  71, 345, 142]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 315,  383,  470,  674],\n",
      "        [ 965,  353, 1062,  652],\n",
      "        [ 434,  460,  462,  502]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[360, 380, 450, 621],\n",
      "        [822, 348, 918, 606],\n",
      "        [386, 440, 430, 476]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[297, 438, 380, 585],\n",
      "        [665, 353, 773, 606],\n",
      "        [290, 233, 325, 285],\n",
      "        [672, 405, 695, 431]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[264, 377, 356, 571],\n",
      "        [620, 354, 746, 608],\n",
      "        [240, 507, 275, 550],\n",
      "        [632, 445, 656, 470]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 27, 596,  65, 631],\n",
      "        [ 69, 390, 182, 608],\n",
      "        [641, 352, 706, 605]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[547, 357, 616, 588],\n",
      "        [542, 470, 560, 489]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[195, 260, 234, 297],\n",
      "        [371, 349, 506, 586]], device='cuda:0'), 'labels': tensor([1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[137, 252, 181, 291],\n",
      "        [300, 343, 428, 589]], device='cuda:0'), 'labels': tensor([1, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[526, 441, 556, 474],\n",
      "        [521, 359, 595, 601],\n",
      "        [  0, 484,  13, 515]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[622, 444, 655, 477],\n",
      "        [ 67, 387, 189, 640],\n",
      "        [625, 360, 700, 604]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[697, 477, 729, 509],\n",
      "        [205, 403, 270, 600],\n",
      "        [705, 369, 777, 619]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[741, 414, 759, 433],\n",
      "        [503, 394, 568, 517],\n",
      "        [749, 375, 791, 514]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[873, 403, 894, 422],\n",
      "        [742, 390, 805, 507],\n",
      "        [879, 379, 931, 516]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[ 489,  418,  506,  434],\n",
      "        [ 965,  363, 1023,  467],\n",
      "        [ 450,  391,  499,  508]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[300, 388, 334, 508],\n",
      "        [823, 361, 850, 454],\n",
      "        [386, 310, 401, 323],\n",
      "        [320, 407, 328, 414]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[  0, 388,  45, 523],\n",
      "        [538, 365, 578, 449],\n",
      "        [515, 412, 528, 431]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[310, 294, 329, 311],\n",
      "        [ 47, 389, 129, 526],\n",
      "        [635, 360, 664, 452]], device='cuda:0'), 'labels': tensor([1, 0, 0], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[484, 370, 523, 444],\n",
      "        [469, 404, 483, 417]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[362, 344, 413, 447],\n",
      "        [366, 371, 384, 386]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([], device='cuda:0', size=(0, 4), dtype=torch.int64), 'labels': tensor([], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[560, 364, 603, 448],\n",
      "        [396, 277, 410,  29]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([[904, 362, 944, 470],\n",
      "        [209, 416, 302, 531],\n",
      "        [143, 475, 167, 497]], device='cuda:0'), 'labels': tensor([0, 0, 1], device='cuda:0', dtype=torch.int32)}, {'boxes': tensor([], device='cuda:0', size=(0, 4), dtype=torch.int64), 'labels': tensor([], device='cuda:0', dtype=torch.int32)}]\n",
      "Inside mAP: 47.52%\n",
      "Inside mAP (person): 61.95%\n",
      "Inside mAP (ball): 33.09%\n",
      "Outside mAP: 50.18%\n",
      "Outside mAP (person): 91.45%\n",
      "Outside mAP (ball): 8.91%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "label_ids = [\n",
    "\t[115, 156, 212, 275, 320, 368, 376, 430, 492, 550, 600, 668, 725, 773, 815, 873, 940, 994, 1055, 1100, 1165, 1205, 1275, 1385, 1494],\n",
    "\t[112, 167, 200, 260, 300, 344, 406, 482, 533, 649, 708, 761, 954, 988, 1120, 1165, 1203, 1313, 1345, 1378, 1401, 1425, 1469, 1499]\n",
    "]\n",
    "\n",
    "def eval_map(scene, debug_inf_time=False, rgb=False):\n",
    "\t# Read reference data and build dictionary\n",
    "\tids = label_ids[scene - 1]\n",
    "\n",
    "\t# Read file box_5_1.txt as csv\n",
    "\tdf = pd.read_csv(os.path.join('data', 'box_5_{}.txt'.format(scene)), header=None, sep=', ', engine='python')\n",
    "\tdf.columns = ['id', 'x1', 'y1', 'x2', 'y2', 'class']\n",
    "\tdf.replace({'class': {'person': 0, 'ball': 1}}, inplace=True)\n",
    "\tdf['id'] = df['id'].apply(lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "\tpreds = []\n",
    "\ttarget = []\n",
    "\tinf_time = []\n",
    "\n",
    "\tfor id in ids:\n",
    "\t\t# Read frame and predict\n",
    "\t\timg_path = os.path.join('data', 'img_5_{}'.format(scene), 'img_5_{}_{}.jpg'.format(scene, str(id).zfill(4)))\n",
    "\t\timg = cv2.imread(img_path, cv2.IMREAD_COLOR if rgb else cv2.IMREAD_GRAYSCALE)\n",
    "\t\tt = time.time()\n",
    "\t\tpred = md.detect(img)['boxes']\n",
    "\t\tinf_time.append(time.time() - t)\n",
    "\t\t# pred is a tensor of shape (N, 6) where N is the number of bounding boxes, and the 6 values are (x1, y1, x2, y2, conf, class)\n",
    "\n",
    "\t\tboxes = pred[:, :4]\n",
    "\t\tscores = pred[:, 4]\n",
    "\t\tlabels = pred[:, 5].int()\n",
    "\n",
    "\t\tpreds.append({'boxes': boxes, 'scores': scores, 'labels': labels})\n",
    "\n",
    "\t\t# Get ground truth\n",
    "\t\tgt = df[df['id'] == id]\n",
    "\t\tgt = torch.from_numpy(gt[['x1', 'y1', 'x2', 'y2', 'class']].values).to(device)\n",
    "\n",
    "\t\tboxes = gt[:, :4]\n",
    "\t\tlabels = gt[:, 4].int()\n",
    "\n",
    "\t\ttarget.append({'boxes': boxes, 'labels': labels})\n",
    "\t\t\n",
    "\tmetric = MeanAveragePrecision(iou_thresholds=[0.5], class_metrics=True).to(device)\n",
    "\tmetric.update(preds, target)\n",
    "\tresults = metric.compute()\n",
    "\n",
    "\tif debug_inf_time:\n",
    "\t\tprint('Average inference time: {}s'.format(np.mean(inf_time)))\n",
    "\n",
    "\treturn (results['map_50'].item(), *results['map_per_class'].tolist())\n",
    "\n",
    "md = MotionDetector(model='yolov5n')\n",
    "rgb = True\n",
    "inside_map, inside_map_person, inside_map_ball = eval_map(1, rgb=rgb)\n",
    "outside_map, outside_map_person, outside_map_ball = eval_map(2, rgb=rgb)\n",
    "print('Inside mAP: {}%'.format(round(inside_map * 100, 2)))\n",
    "print('Inside mAP (person): {}%'.format(round(inside_map_person * 100, 2)))\n",
    "print('Inside mAP (ball): {}%'.format(round(inside_map_ball * 100, 2)))\n",
    "print('Outside mAP: {}%'.format(round(outside_map * 100, 2)))\n",
    "print('Outside mAP (person): {}%'.format(round(outside_map_person * 100, 2)))\n",
    "print('Outside mAP (ball): {}%'.format(round(outside_map_ball * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed\n",
    "\n",
    "This module helps to compare the speed of the different detection models we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/sacha/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-12-2 Python-3.9.0 torch-1.13.0 CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/sacha/miniconda3/envs/tfe/lib/python3.9/site-packages/certifi-2022.9.24.dist-info/METADATA'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in /Users/sacha/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-12-2 Python-3.9.0 torch-1.13.0 CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:03<00:00, 3.72MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time for models:\n",
      "yolov5n: 115.31ms (8.67fps)\n",
      "yolov5s: 217.68ms (4.59fps)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from motion_detection import MotionDetector\n",
    "from motion_detection import MODELS\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "\n",
    "path = \"data/img_5_1/img_5_1_\"\n",
    "nb_frames = 500\n",
    "\n",
    "avg_times = dict()\n",
    "for model in ['yolov5n', 'yolov5s']: \n",
    "    md = MotionDetector(model=model)\n",
    "    inf_times = []\n",
    "    for i in range(0, nb_frames):\n",
    "        img = cv2.imread(path + str(i).zfill(4) + \".jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "        t = time.time()\n",
    "        md.detect(img)\n",
    "        inf_times.append(time.time() - t)\n",
    "    avg_times[model] = (np.mean(inf_times) * 1000).round(2)\n",
    "\n",
    "print('Average inference time for models:')\n",
    "for model, avg_time in avg_times.items():\n",
    "    print('{}: {}ms ({}fps)'.format(model, avg_time, round(1000 / avg_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ball Tracking\n",
    "\n",
    "Here, the performance of the object tracker is assessed.\n",
    "\n",
    "The tracker outputs at each frame the position and depth ((x,y) and z) for the ball(s).\n",
    "\n",
    "The ball center on the x and y axes is supposed to be the center of the bounding box. For performance about this, refer to Object Detection, which evaluates those bounding boxes.\n",
    "\n",
    "***TODO*** For the depth, we record some frames and measure the physical depth, and then average the distance between the predicted and ground truth depth on all annotated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
